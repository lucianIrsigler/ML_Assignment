{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2614f2ae-6ba1-4263-894b-a2c93a6cf249",
   "metadata": {},
   "source": [
    "To see what the data is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1971,
   "id": "15cb9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 65\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1972,
   "id": "afe1fe7b-919a-4f33-9c47-183d25bd174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "import random\n",
    "\n",
    "def displayImage():\n",
    "    with open(\"traindata.txt\",\"r\") as f:\n",
    "        data = f.readlines()\n",
    "        index = random.randint(0,len(data))\n",
    "        index = 3\n",
    "\n",
    "        data = [float(i) for i in data[index].split(',')] # Your list of 1040 grayscale values followed by the rotation angle (as strings)\n",
    "\n",
    "        # Ensure the length of the data list is correct\n",
    "        if len(data) != 1041:\n",
    "            raise ValueError(\"The data list must contain exactly 1041 values, with the last being the rotation angle.\")\n",
    "        \n",
    "        # Separate the pixel values and the rotation angle\n",
    "        rotation_angle = float(data[-1])\n",
    "        pixels = [int(p) for p in data[:-1]]\n",
    "        \n",
    "        # Reshape the pixels array to match the desired dimensions, e.g., 40x26\n",
    "        width, height = 40, 26\n",
    "        image_array = np.array(pixels).reshape((height, width))\n",
    "        \n",
    "        # Apply the rotation to the image\n",
    "        rotated_image = scipy.ndimage.rotate(image_array, rotation_angle, reshape=True)\n",
    "        \n",
    "        # Display the image using Matplotlib\n",
    "        plt.imshow(rotated_image, cmap='gray', vmin=0, vmax=255)\n",
    "        plt.colorbar()  # Optional: adds a colorbar to indicate the scale\n",
    "        plt.title(f\"Grayscale Image {index}(Rotated {rotation_angle} degrees)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fcaf2b1-d43e-4898-ba3b-6b9a4b28ea35",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "The dataset:\n",
    "1041 features\n",
    "1040 pixels\n",
    "1 rotation(last feature)\n",
    "5250 inputs\n",
    "Training labels are [0,20]\n",
    "Only rotations are [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1973,
   "id": "bce1ad6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_one_encoding(number):\n",
    "    nums = [0]*4\n",
    "    nums[number]=1\n",
    "    return nums\n",
    "\n",
    "def min_max_scale(trainData,trainLabels):\n",
    "    pixel_arrays = []\n",
    "    pixels_labels = []\n",
    "    for i in range(len(trainData)):\n",
    "        pixels = list(map(float,trainData[i]))\n",
    "        rotation_angle = pixels.pop()\n",
    "        hotOneEncoding = hot_one_encoding(int(rotation_angle))\n",
    "\n",
    "        min_val = min(pixels)\n",
    "        max_val = max(pixels)\n",
    "        \n",
    "        for j in range(len(pixels)):\n",
    "            pixels[j] = (pixels[j]-min_val)/(max_val-min_val)\n",
    "\n",
    "        for j in hotOneEncoding:\n",
    "            pixels.append(j)\n",
    "\n",
    "        pixel_arrays.append(pixels)\n",
    "        pixels_labels.append(int(trainLabels[i]))\n",
    "\n",
    "    inputData = np.array(pixel_arrays)\n",
    "    inputLabels = np.array(pixels_labels)\n",
    "\n",
    "    return inputData,inputLabels\n",
    "\n",
    "def noMinMax(trainData,trainLabels):\n",
    "    pixel_arrays = []\n",
    "    pixels_labels = []\n",
    "\n",
    "    for i in range(len(trainData)):\n",
    "        pixels = list(map(float,trainData[i]))\n",
    "        rotation_angle = pixels.pop()\n",
    "        hotOneEncoding = hot_one_encoding(int(rotation_angle))\n",
    "        for j in hotOneEncoding:\n",
    "            pixels.append(j)\n",
    "        pixel_arrays.append(pixels)\n",
    "        pixels_labels.append(int(trainLabels[i]))\n",
    "    inputData = np.array(pixel_arrays)\n",
    "    inputLabels = np.array(pixels_labels)\n",
    "\n",
    "    return inputData,inputLabels\n",
    "\n",
    "def remove_outliers_iqr(inputData, inputLabels):\n",
    "    Q1 = np.percentile(inputData, 5, axis=0)\n",
    "    Q3 = np.percentile(inputData, 95, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    non_outlier_mask = np.all((inputData >= (Q1 - 1.5 * IQR)) & (inputData <= (Q3 + 1.5 * IQR)), axis=1)\n",
    "    \n",
    "    filteredData = inputData[non_outlier_mask]\n",
    "    filteredLabels = inputLabels[non_outlier_mask]\n",
    "    \n",
    "    return filteredData, filteredLabels\n",
    "\n",
    "def remove_columns_with_negative_first(inputData):\n",
    "    \n",
    "    inputData = np.array(inputData)\n",
    "    \n",
    "    columns_to_keep = [col for col in range(inputData.shape[1]) if inputData[0, col] >= 0]\n",
    "    \n",
    "    filteredData = inputData[:, columns_to_keep]\n",
    "    \n",
    "    return filteredData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1974,
   "id": "ba9fa76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"traindata.txt\",\"r\") as f:\n",
    "    trainData = [i.split(\",\") for i in f.readlines()]\n",
    "\n",
    "with open(\"trainlabels.txt\",\"r\") as f:\n",
    "    trainLabels = f.readlines()\n",
    "\n",
    "inputData,inputLabels = min_max_scale(trainData,trainLabels)\n",
    "#inputData = remove_columns_with_negative_first(inputData)    \n",
    "inputData,inputLabels = remove_outliers_iqr(inputData,inputLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1975,
   "id": "3c8bb45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutlier(inputData1,inputLabels1):\n",
    "    removeRow = False\n",
    "    if (removeRow):\n",
    "        rows = [ 68,  202,  234,  442,  479,  553,  564,  589, 1120, 1242, 1357, 1584,\n",
    "        2017, 2588, 2724, 3117, 3396, 3414, 3603, 3747, 3845, 3916, 4351, 4479,\n",
    "        4496, 4604, 4692, 4711, 4806, 4830]\n",
    "        inputData1 = np.delete(inputData1,rows,axis=0)\n",
    "        inputLabels1 = np.delete(inputLabels1,rows,axis=0)\n",
    "    else:\n",
    "        cols=[ 89, 172, 189, 210, 236, 287, 367, 383, 416, 448, 455, 460, 462, 467,\n",
    "        531, 546, 565, 574, 653, 666, 678, 702, 723, 726, 764, 787, 791, 805,\n",
    "        810, 818, 856, 861, 873, 892, 894, 924, 929, 964]\n",
    "        \n",
    "        inputData1 = np.delete(inputData, cols, axis=1)\n",
    "\n",
    "    return inputData1,inputLabels1\n",
    "\n",
    "#inputData,inputLabels = removeOutlier(inputData,inputLabels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1976,
   "id": "87adb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_temp,y_train,y_temp = train_test_split(inputData,inputLabels,test_size=0.2,random_state=seed)\n",
    "x_test, x_val,y_test,y_val, = train_test_split(x_temp,y_temp, test_size=0.5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1977,
   "id": "c20847b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1978,
   "id": "fcbc4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainingCount():\n",
    "    count = {i:0 for i in range(21)}\n",
    "\n",
    "    for i in trainLabels:\n",
    "        count[int(i)]+=1\n",
    "\n",
    "    return count\n",
    "\n",
    "data = getTrainingCount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1979,
   "id": "b435c82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25e1429f9b0>"
      ]
     },
     "execution_count": 1979,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1980,
   "id": "0c8a563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1981,
   "id": "6b2a6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "x_val = torch.FloatTensor(x_val)\n",
    "\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "y_val = torch.LongTensor(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1982,
   "id": "fd463c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    #h1=512,h2=128,out=21 => 60% with dropout of 50% ;in was 1044\n",
    "    def __init__(self,in_features=1044,h1=512,h2=128,out=21):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_features,h1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(h1,h2),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(0.25)\n",
    "        )\n",
    "\n",
    "        self.outlayer = nn.Sequential(\n",
    "            nn.Linear(h2,out)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.outlayer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1983,
   "id": "3c27aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\":1000,\n",
    "    \"checkInterval\":5,\n",
    "    \"printInterval\":30,\n",
    "    \"patience\":30\n",
    "}\n",
    "\n",
    "outputConfig = {\n",
    "    \"maxEpochs\":1000,\n",
    "    \"losses\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1984,
   "id": "63d12c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(newModel,canPrint=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(newModel.parameters(),lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0\n",
    "    temp_losses = []\n",
    "    temp_epoch = 1000\n",
    "\n",
    "    best_model_state = newModel.state_dict()\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        newModel.train()\n",
    "        y_pred = newModel(x_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        temp_losses.append(loss.item())  # Append the loss value as a Python scalar\n",
    "        if (epoch % config[\"printInterval\"] == 0 and canPrint == True):\n",
    "            print(f\"Epoch: {epoch} and loss: {loss.item()}\\t No improvement:{no_improvement_count}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if (epoch % config[\"checkInterval\"] == 0):\n",
    "            newModel.eval()\n",
    "            with torch.no_grad():\n",
    "                y_val_pred = newModel(x_val)\n",
    "                val_loss = criterion(y_val_pred, y_val)\n",
    "                if val_loss.item() < best_val_loss:\n",
    "                    best_val_loss = val_loss.item()\n",
    "                    no_improvement_count = 0  # Reset the no improvement count\n",
    "                    best_model_state = newModel.state_dict()\n",
    "                else:\n",
    "                    no_improvement_count += 1\n",
    "                    \n",
    "                    if no_improvement_count >= config[\"patience\"]:\n",
    "                        temp_epoch = epoch\n",
    "                        newModel.load_state_dict(best_model_state)\n",
    "                        break\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    if (canPrint==True):\n",
    "        print(f\"Best epoch: {temp_epoch}\")\n",
    "    \n",
    "    outputConfig[\"losses\"] = temp_losses\n",
    "    outputConfig[\"maxEpochs\"] = temp_epoch\n",
    "    return newModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1985,
   "id": "8df0d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(newModel):\n",
    "    newModel= train(newModel,True)\n",
    "    return newModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1986,
   "id": "5e9586b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph out loss for model\n",
    "def graph():\n",
    "    plt.plot(range(outputConfig[\"maxEpochs\"]),outputConfig[\"losses\"])\n",
    "    plt.ylabel(\"loss/error\")\n",
    "    plt.xlabel(\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1987,
   "id": "e55f8fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "incorrect = {i: 0 for i in range(21)}  # Initialize the nested dictionary\n",
    "\n",
    "def correct(model):\n",
    "    incorrect = {i: 0 for i in range(21)} \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_val = model.forward(x_test)\n",
    "        for i in range(len(y_test)):\n",
    "            predicted_label = y_val[i].argmax().item()\n",
    "            true_label = y_test[i].item()\n",
    "\n",
    "            if predicted_label == true_label:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect[true_label] += 1\n",
    "\n",
    "    accuracy = correct / len(y_test)\n",
    "    return accuracy\n",
    "\n",
    "def saveIncorrectToJSON(): \n",
    "    with open('incorrect_predictions.json', 'w') as json_file:\n",
    "        json.dump(incorrect, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1988,
   "id": "de241b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 and loss: 3.110607385635376\t No improvement:0\n",
      "Epoch: 30 and loss: 2.9885177612304688\t No improvement:0\n",
      "Epoch: 60 and loss: 2.668943166732788\t No improvement:0\n",
      "Epoch: 90 and loss: 2.501100778579712\t No improvement:0\n",
      "Epoch: 120 and loss: 2.35137677192688\t No improvement:1\n",
      "Epoch: 150 and loss: 2.236222505569458\t No improvement:0\n",
      "Epoch: 180 and loss: 2.152705669403076\t No improvement:1\n",
      "Epoch: 210 and loss: 1.983105182647705\t No improvement:1\n",
      "Epoch: 240 and loss: 1.859483003616333\t No improvement:1\n",
      "Epoch: 270 and loss: 1.689530611038208\t No improvement:0\n",
      "Epoch: 300 and loss: 1.5764365196228027\t No improvement:0\n",
      "Epoch: 330 and loss: 1.457172155380249\t No improvement:0\n",
      "Epoch: 360 and loss: 1.3548959493637085\t No improvement:0\n",
      "Epoch: 390 and loss: 1.243454933166504\t No improvement:0\n",
      "Epoch: 420 and loss: 1.1513111591339111\t No improvement:0\n",
      "Epoch: 450 and loss: 1.0804418325424194\t No improvement:0\n",
      "Epoch: 480 and loss: 1.004408836364746\t No improvement:0\n",
      "Epoch: 510 and loss: 0.9411993026733398\t No improvement:3\n",
      "Epoch: 540 and loss: 0.8908483386039734\t No improvement:5\n",
      "Epoch: 570 and loss: 0.8416550755500793\t No improvement:1\n",
      "Epoch: 600 and loss: 0.8042693138122559\t No improvement:7\n",
      "Epoch: 630 and loss: 0.7427000403404236\t No improvement:13\n",
      "Epoch: 660 and loss: 0.7002578973770142\t No improvement:19\n",
      "Epoch: 690 and loss: 0.6607325673103333\t No improvement:25\n",
      "Best epoch: 710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5733333333333334"
      ]
     },
     "execution_count": 1988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newModel = Model()\n",
    "optimizer = torch.optim.Adam(newModel.parameters(),lr=learning_rate)\n",
    "newModel = trainModel(newModel)\n",
    "accuracy = correct(newModel)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1989,
   "id": "3960b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMultiple():\n",
    "    in_features_const = 1044\n",
    "    #h1:h2\n",
    "    layers = [\n",
    "        (520, 256),\n",
    "        (512, 256), (512, 128), (512, 64), (512, 32),\n",
    "        (256, 128), (256, 64), (256, 32),\n",
    "        (1024, 512), (1024, 256), (1024, 128),\n",
    "    ]\n",
    "\n",
    "    for i in layers:\n",
    "        h1 = i[0]\n",
    "        h2 = i[1]\n",
    "        print(f\"h1:{h1} and h2:{h2}\")\n",
    "        newModel = Model(in_features_const,h1,h2)\n",
    "        newModel = trainModel(newModel)\n",
    "        correct(newModel)\n",
    "\n",
    "#testMultiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1990,
   "id": "f8500699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "def save(model):\n",
    "    torch.save(model.state_dict(),\"classify.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
